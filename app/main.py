import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from slowapi import _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded

import time
from .api import auth, proxy, analytics, providers, cache, smart_routing, alerts, rate_limits, monitoring, dashboard, schemas, public_benchmarks
from .api.monitoring import request_count, request_latency
from .database import Base, engine
from .services.background_tasks import scheduler
from .limiter import limiter
from .config import get_settings

settings = get_settings()
if settings.SENTRY_DSN:
    sentry_sdk.init(
        dsn=settings.SENTRY_DSN,
        environment=settings.ENVIRONMENT,
        integrations=[
            FastApiIntegration(),
        ],
        traces_sample_rate=0.2,
    )


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Auto-run migrations on startup
    import subprocess
    import sys
    
    try:
        # Run Alembic migrations
        print("Running database migrations...")
        result = subprocess.run(
            [sys.executable, "-m", "alembic", "upgrade", "head"],
            capture_output=True,
            text=True,
            cwd="/Users/billy/Documents/Projects/cognitude_mvp"
        )
        
        if result.returncode == 0:
            print("‚úÖ Database migrations completed successfully")
            print(result.stdout)
        else:
            print("‚ùå Database migrations failed:")
            print(result.stderr)
            # Don't raise exception - allow app to start even if migrations fail
            # This is important for development and first-time setup
    except Exception as e:
        print(f"‚ùå Error running migrations: {e}")
        print("Continuing with application startup...")
    
    scheduler.start()
    yield
    scheduler.shutdown()


app = FastAPI(
    title="Cognitude LLM Proxy",
    description="""
## LLM Proxy with Intelligent Caching & Multi-Provider Routing

Cognitude is an OpenAI-compatible LLM proxy that provides intelligent caching, multi-provider routing, and comprehensive cost analytics.

### ‚ú® Key Features

* **üîÑ OpenAI Compatible**: Drop-in replacement for OpenAI SDK
* **üíæ Intelligent Caching**: 30-70% cost savings through response caching
* **üåê Multi-Provider**: Support for OpenAI, Anthropic, Mistral, Groq
* **üí∞ Cost Tracking**: Accurate per-request cost calculation
* **üìä Analytics**: Comprehensive usage metrics and insights
* **üîê Multi-Tenant**: Secure API key-based organization isolation

### üöÄ Quick Start

```python
from openai import OpenAI

# Use Cognitude instead of OpenAI directly
client = OpenAI(
    api_key="your-cognitude-api-key",
    base_url="http://your-server:8000/v1"
)

# Same OpenAI code works!
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### üìñ API Endpoints

* **Chat Completions**: `POST /v1/chat/completions` - OpenAI-compatible LLM endpoint
* **List Models**: `GET /v1/models` - Available models based on your providers
* **Analytics**: `GET /analytics/usage` - Usage metrics and cost breakdown
* **Providers**: `/providers/*` - Manage LLM provider configurations
* **Cache**: `/cache/*` - Cache management and statistics

### üîê Authentication

All endpoints require an API key in the `X-API-Key` header:

```bash
curl -X POST http://your-server:8000/v1/chat/completions \\
  -H "X-API-Key: your-api-key" \\
  -H "Content-Type: application/json" \\
  -d '{"model": "gpt-3.5-turbo", "messages": [...]}'
```
    """,
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    contact={
        "name": "Cognitude Support",
        "url": "https://cognitude.io/support",
        "email": "support@cognitude.io"
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT",
    },
    lifespan=lifespan
)

app.state.limiter = limiter

@app.exception_handler(RateLimitExceeded)
async def rate_limit_exceeded_handler(request: Request, exc: RateLimitExceeded):
    return _rate_limit_exceeded_handler(request, exc)

# Configure CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow frontend origin
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.middleware("http")
async def track_metrics(request: Request, call_next):
    # Skip metrics for health checks
    if request.url.path in ["/", "/health"]:
        return await call_next(request)
    
    start_time = time.time()
    try:
        response = await call_next(request)
        process_time = time.time() - start_time
        
        endpoint = request.url.path
        status_code = response.status_code
        
        request_count.labels(method=request.method, endpoint=endpoint, status=status_code).inc()
        request_latency.labels(method=request.method, endpoint=endpoint).observe(process_time)
        
        return response
    except Exception as e:
        process_time = time.time() - start_time
        request_count.labels(method=request.method, endpoint=request.url.path, status=500).inc()
        request_latency.labels(method=request.method, endpoint=request.url.path).observe(process_time)
        raise

# Include routers
app.include_router(auth.router, prefix="/auth", tags=["auth"])
# Core functionality
app.include_router(proxy.router, tags=["proxy"])
app.include_router(providers.router, tags=["providers"])

# Monitoring & Analytics
app.include_router(monitoring.router, tags=["monitoring"])
app.include_router(analytics.router, tags=["analytics"])

# Configuration
app.include_router(rate_limits.router, tags=["rate-limits"])
app.include_router(alerts.router, tags=["alerts"])
app.include_router(smart_routing.router)  # Smart routing endpoints
app.include_router(cache.router)  # Uses /cache prefix from router
app.include_router(dashboard.router, prefix="/api/v1/dashboard", tags=["dashboard"])
app.include_router(schemas.router, prefix="/api/schemas", tags=["schemas"])
app.include_router(public_benchmarks.router)

@app.get("/health")
def health_check():
    return {"status": "healthy"}

@app.get("/")
def read_root():
    return {
        "message": "Cognitude LLM Proxy",
        "version": "1.0.0",
        "docs": "/docs"
    }
